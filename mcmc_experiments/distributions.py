from abc import ABC, abstractmethod
import torch
from torch import Tensor
from torch.distributions import MultivariateNormal, Categorical


class DataDistribution(ABC):
    """Abstract base class for data distributions."""

    @property
    @abstractmethod
    def dim(self) -> int:
        """Returns the dimension of the data generated by this distribution."""

    @abstractmethod
    def sample(self, num_samples: int) -> Tensor:
        """
        Draws `num_samples` samples from the data distribution.

        Args:
            num_samples: Number of samples to generate.

        Returns:
            Tensor of shape (num_samples, self.dim).
        """

    @abstractmethod
    def likelihood(self, data: Tensor) -> Tensor:
        """
        Computes the likelihood of a batch of data points.

        Args:
            data: Tensor of shape (*, self.dim).

        Returns:
            Tensor of shape (*), representing the likelihood of each data point.
        """

    @abstractmethod
    def score(self, data: Tensor) -> Tensor:
        """
        Computes the score (gradient of log-likelihood) at a batch of data points.

        Args:
            data: Tensor of shape (*, self.dim), where the score is evaluated.

        Returns:
            Tensor of shape (*, self.dim), representing the score at each point.
        """


class GaussianMixtureModel(DataDistribution):
    def __init__(self, means: Tensor, covariances: Tensor, weights: Tensor):
        """
        Initialize a Gaussian Mixture Model (GMM) with batched parameters.

        Args:
            means: Tensor of shape (num_components, dim) representing the means.
            covariances: Tensor of shape (num_components, dim, dim) representing the covariances.
            weights: Tensor of shape (num_components,) representing the mixture weights (must sum to 1).
        """
        assert (
            means.shape[0] == covariances.shape[0] == weights.shape[0]
        ), "Number of components must match across means, covariances, and weights."

        assert torch.isclose(
            weights.sum(), torch.ones(1)
        ), "Weights vector must sum to one."

        assert (
            len(means.shape) == 2
        ), f"means must be two-dimensional, got {len(means.shape)}."
        num_components, dim = means.shape
        assert (
            covariances.shape
            == (
                num_components,
                dim,
                dim,
            )
        ), f"covariances must have shape {(num_components, dim, dim)}, got {covariances.shape}"

        assert weights.shape == (
            num_components,
        ), f"weights must have shape {(num_components,)}, got {weights.shape}"

        self.means = means  # Shape: (num_components, dim)
        self.covariances = covariances  # Shape: (num_components, dim, dim)
        self.weights = weights  # Shape: (num_components,)

        # Create a single batched MultivariateNormal distribution
        self.mvn = MultivariateNormal(self.means, self.covariances)
        self.num_components = means.shape[0]
        self._dim = means.shape[1]

    @property
    def dim(self) -> int:
        return self._dim

    def sample(self, num_samples: int) -> Tensor:
        """
        Draw samples from the GMM.

        Args:
            num_samples: Number of samples to generate.

        Returns:
            Tensor of shape (num_samples, dim).
        """
        # Sample component indices for each data point
        component_indices = Categorical(self.weights).sample((num_samples,))

        # Gather means and covariances for the selected components
        selected_means = self.means[component_indices]  # Shape: (num_samples, dim)
        selected_covs = self.covariances[
            component_indices
        ]  # Shape: (num_samples, dim, dim)

        # Create a distribution for the selected components and sample
        selected_mvn = MultivariateNormal(selected_means, selected_covs)
        return selected_mvn.rsample()  # Shape: (num_samples, dim)

    def log_likelihood(self, data: Tensor) -> Tensor:
        """
        Compute the log likelihood of the given data under the GMM.

        Args:
            data: Tensor of shape (*, dim).

        Returns:
            Tensor of shape (*), representing the log likelihood of each data point.
        """
        # Compute the log-probabilities for all components and data points
        log_probs = self.mvn.log_prob(data.unsqueeze(1))  # Shape: (*, num_components)

        # Weight the log-probabilities by the log of the mixture weights
        weighted_log_probs = log_probs + torch.log(
            self.weights
        )  # Shape: (*, num_components)

        # Use logsumexp to compute the log of the total weighted likelihood
        log_likelihood = torch.logsumexp(weighted_log_probs, dim=1)  # Shape: (*,)

        # Exponentiate to get the true likelihood in non-log space
        return log_likelihood  # Shape: (*,)

    def likelihood(self, data: Tensor) -> Tensor:
        """
        Compute the true likelihood of the given data under the GMM.

        Args:
            data: Tensor of shape (*, dim).

        Returns:
            Tensor of shape (*), representing the likelihood of each data point.
        """
        return self.log_likelihood(data).exp()

    def score(self, data: Tensor) -> Tensor:
        """
        Compute the score (gradient of log-likelihood) for each data point.

        Args:
            data: Tensor of shape (*, dim).

        Returns:
            Tensor of shape (*, dim), representing the score for each data point.
        """
        data = data.clone().detach().requires_grad_(True)
        log_likelihood = self.log_likelihood(data).sum()
        log_likelihood.backward()
        return data.grad  # Shape: (*, dim)
